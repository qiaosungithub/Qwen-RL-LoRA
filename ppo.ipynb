{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Notebook\n",
    "\n",
    "Implement a simple SFT and PPO training pipeline for finetuning Qwen2.5-7B model on GSM8K dataset.\n",
    "\n",
    "1. Load the Qwen2.5-7B model and tokenizer.\n",
    "2. Load the GSM8K dataset from `openai/gsm8k`.\n",
    "3. Split the dataset into training and validation sets.\n",
    "4. Implement Supervised Fine-Tuning (SFT) on the training set using peft (LoRA).\n",
    "5. Implement Proximal Policy Optimization (PPO) on the SFT model using trl."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***TODO***:\n",
    "\n",
    "1. Fix bugs\n",
    "2. Save trained models to specific paths\n",
    "3. Set up GPU devices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/bowenyu/miniconda3/envs/rl-lora/lib/python3.10/site-packages (4.57.1)\n",
      "Requirement already satisfied: datasets in /home/bowenyu/miniconda3/envs/rl-lora/lib/python3.10/site-packages (4.0.0)\n",
      "Collecting datasets\n",
      "  Using cached datasets-4.4.1-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.6-py3-none-any.whl.metadata (9.5 kB)\n",
      "Requirement already satisfied: peft in /home/bowenyu/miniconda3/envs/rl-lora/lib/python3.10/site-packages (0.17.1)\n",
      "Collecting peft\n",
      "  Downloading peft-0.18.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: trl in /home/bowenyu/miniconda3/envs/rl-lora/lib/python3.10/site-packages (0.25.1)\n",
      "Collecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.48.2-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: filelock in /home/bowenyu/miniconda3/envs/rl-lora/lib/python3.10/site-packages (from transformers) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /home/bowenyu/miniconda3/envs/rl-lora/lib/python3.10/site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/bowenyu/miniconda3/envs/rl-lora/lib/python3.10/site-packages (from transformers) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/bowenyu/miniconda3/envs/rl-lora/lib/python3.10/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/bowenyu/miniconda3/envs/rl-lora/lib/python3.10/site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/bowenyu/miniconda3/envs/rl-lora/lib/python3.10/site-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in /home/bowenyu/miniconda3/envs/rl-lora/lib/python3.10/site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /home/bowenyu/miniconda3/envs/rl-lora/lib/python3.10/site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/bowenyu/miniconda3/envs/rl-lora/lib/python3.10/site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/bowenyu/miniconda3/envs/rl-lora/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/bowenyu/miniconda3/envs/rl-lora/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/bowenyu/miniconda3/envs/rl-lora/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/bowenyu/miniconda3/envs/rl-lora/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Collecting pyarrow>=21.0.0 (from datasets)\n",
      "  Using cached pyarrow-22.0.0.tar.gz (1.2 MB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: dill<0.4.1,>=0.3.0 in /home/bowenyu/miniconda3/envs/rl-lora/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/bowenyu/miniconda3/envs/rl-lora/lib/python3.10/site-packages (from datasets) (2.3.3)\n",
      "Requirement already satisfied: httpx<1.0.0 in /home/bowenyu/miniconda3/envs/rl-lora/lib/python3.10/site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: xxhash in /home/bowenyu/miniconda3/envs/rl-lora/lib/python3.10/site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in /home/bowenyu/miniconda3/envs/rl-lora/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/bowenyu/miniconda3/envs/rl-lora/lib/python3.10/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.2)\n",
      "Requirement already satisfied: anyio in /home/bowenyu/miniconda3/envs/rl-lora/lib/python3.10/site-packages (from httpx<1.0.0->datasets) (4.11.0)\n",
      "Requirement already satisfied: certifi in /home/bowenyu/miniconda3/envs/rl-lora/lib/python3.10/site-packages (from httpx<1.0.0->datasets) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in /home/bowenyu/miniconda3/envs/rl-lora/lib/python3.10/site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in /home/bowenyu/miniconda3/envs/rl-lora/lib/python3.10/site-packages (from httpx<1.0.0->datasets) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /home/bowenyu/miniconda3/envs/rl-lora/lib/python3.10/site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: psutil in /home/bowenyu/miniconda3/envs/rl-lora/lib/python3.10/site-packages (from peft) (7.1.3)\n",
      "Requirement already satisfied: torch>=1.13.0 in /home/bowenyu/miniconda3/envs/rl-lora/lib/python3.10/site-packages (from peft) (2.6.0)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /home/bowenyu/miniconda3/envs/rl-lora/lib/python3.10/site-packages (from peft) (1.11.0)\n",
      "Requirement already satisfied: networkx in /home/bowenyu/miniconda3/envs/rl-lora/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/bowenyu/miniconda3/envs/rl-lora/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/bowenyu/miniconda3/envs/rl-lora/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/bowenyu/miniconda3/envs/rl-lora/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/bowenyu/miniconda3/envs/rl-lora/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/bowenyu/miniconda3/envs/rl-lora/lib/python3.10/site-packages (from torch>=1.13.0->peft) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/bowenyu/miniconda3/envs/rl-lora/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/bowenyu/miniconda3/envs/rl-lora/lib/python3.10/site-packages (from torch>=1.13.0->peft) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/bowenyu/miniconda3/envs/rl-lora/lib/python3.10/site-packages (from torch>=1.13.0->peft) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/bowenyu/miniconda3/envs/rl-lora/lib/python3.10/site-packages (from torch>=1.13.0->peft) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/bowenyu/miniconda3/envs/rl-lora/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /home/bowenyu/miniconda3/envs/rl-lora/lib/python3.10/site-packages (from torch>=1.13.0->peft) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/bowenyu/miniconda3/envs/rl-lora/lib/python3.10/site-packages (from torch>=1.13.0->peft) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/bowenyu/miniconda3/envs/rl-lora/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/bowenyu/miniconda3/envs/rl-lora/lib/python3.10/site-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /home/bowenyu/miniconda3/envs/rl-lora/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/bowenyu/miniconda3/envs/rl-lora/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/bowenyu/miniconda3/envs/rl-lora/lib/python3.10/site-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/bowenyu/miniconda3/envs/rl-lora/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /home/bowenyu/miniconda3/envs/rl-lora/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /home/bowenyu/miniconda3/envs/rl-lora/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/bowenyu/miniconda3/envs/rl-lora/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/bowenyu/miniconda3/envs/rl-lora/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/bowenyu/miniconda3/envs/rl-lora/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/bowenyu/miniconda3/envs/rl-lora/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/bowenyu/miniconda3/envs/rl-lora/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/bowenyu/miniconda3/envs/rl-lora/lib/python3.10/site-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/bowenyu/miniconda3/envs/rl-lora/lib/python3.10/site-packages (from requests->transformers) (2.5.0)\n",
      "\u001b[31mERROR: Could not install packages due to an OSError: [Errno 2] No such file or directory: '/home/bowenyu/miniconda3/envs/rl-lora/lib/python3.10/site-packages/certifi-2025.10.5.dist-info/METADATA'\n",
      "\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U transformers datasets evaluate peft trl bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bowenyu/miniconda3/envs/rl-lora/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from datasets import load_dataset\n",
    "from trl import AutoModelForCausalLMWithValueHead, PPOConfig, PPOTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bowenyu/miniconda3/envs/rl-lora/lib/python3.10/site-packages/torch/cuda/__init__.py:129: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11040). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at /pytorch/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]/home/bowenyu/miniconda3/envs/rl-lora/lib/python3.10/site-packages/huggingface_hub/file_download.py:798: UserWarning: Not enough free disk space to download the file. The expected file size is: 3556.38 MB. The target location /home/bowenyu/.cache/huggingface/hub/models--Qwen--Qwen2.5-7B-Instruct/blobs only has 2516.68 MB free disk space.\n",
      "  warnings.warn(\n",
      "/home/bowenyu/miniconda3/envs/rl-lora/lib/python3.10/site-packages/huggingface_hub/file_download.py:798: UserWarning: Not enough free disk space to download the file. The expected file size is: 3864.73 MB. The target location /home/bowenyu/.cache/huggingface/hub/models--Qwen--Qwen2.5-7B-Instruct/blobs only has 2516.68 MB free disk space.\n",
      "  warnings.warn(\n",
      "/home/bowenyu/miniconda3/envs/rl-lora/lib/python3.10/site-packages/huggingface_hub/file_download.py:798: UserWarning: Not enough free disk space to download the file. The expected file size is: 3945.44 MB. The target location /home/bowenyu/.cache/huggingface/hub/models--Qwen--Qwen2.5-7B-Instruct/blobs only has 2516.66 MB free disk space.\n",
      "  warnings.warn(\n",
      "Fetching 4 files:   0%|          | 0/4 [00:11<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.padding_side = \"right\"  # during training, right padding is needed\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ").to(device)  # is it really needed to add \".to(device)\"?\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],  # adjust to Qwen’s naming\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "sft_model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsm8k = load_dataset(\"openai/gsm8k\", \"main\")\n",
    "\n",
    "def format_example(ex):\n",
    "    prompt = (\n",
    "        \"You are a helpful math tutor. Solve the following problem step by step.\\n\"\n",
    "        \"Show your reasoning clearly, and put the final answer in the form \\\"#### <answer>\\\".\\n\\n\"\n",
    "        f\"Question:\\n{ex['question']}\\n\\nAnswer:\\n\"\n",
    "    )\n",
    "    # GSM8K answer already ends with '#### <ans>'\n",
    "    target = ex[\"answer\"]\n",
    "    full_text = prompt + target\n",
    "    return {\"text\": full_text}\n",
    "\n",
    "gsm8k = gsm8k.map(format_example)\n",
    "train_data = gsm8k[\"train\"]\n",
    "test_data = gsm8k[\"test\"]\n",
    "\n",
    "def tokenize_fn(ex):\n",
    "    out = tokenizer(\n",
    "        ex[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=1024,\n",
    "    )\n",
    "    out[\"labels\"] = out[\"input_ids\"].copy()\n",
    "    return out\n",
    "\n",
    "tokenized_train = train_data.map(tokenize_fn, batched=True, remove_columns=train_data.column_names)\n",
    "# tokenized_train = tokenized_train.select(range(5000))  # Optionally limit to first 5000 samples for quicker training\n",
    "tokenized_test = test_data.map(tokenize_fn, batched=True, remove_columns=test_data.column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Fine-Tuning (SFT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./qwen-gsm8k-sft\",\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=8,\n",
    "    num_train_epochs=2,\n",
    "    learning_rate=2e-4,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.03,\n",
    "    bf16=True,\n",
    "    logging_steps=20,\n",
    "    save_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=sft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./qwen-gsm8k-sft-lora\")\n",
    "tokenizer.save_pretrained(\"./qwen-gsm8k-sft-lora\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proximal Policy Optimization (PPO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "\n",
    "# policy model with value head\n",
    "policy_model = AutoModelForCausalLMWithValueHead.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# reference model (frozen)\n",
    "ref_model = AutoModelForCausalLMWithValueHead.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "ref_model.eval()\n",
    "for p in ref_model.parameters():\n",
    "    p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "ppo_config = PPOConfig(\n",
    "    batch_size=64,          # queries per PPO step\n",
    "    forward_batch_size=16,  # microbatching\n",
    "    learning_rate=1e-5,\n",
    "    log_with=None,          # wandb if you want\n",
    "    mini_batch_size=16,\n",
    "    ppo_epochs=4,\n",
    "    kl_penalty=\"kl\",\n",
    "    kl_coef=0.1,\n",
    "    target_kl=0.1,\n",
    ")\n",
    "\n",
    "ppo_trainer = PPOTrainer(\n",
    "    config=ppo_config,\n",
    "    model=policy_model,\n",
    "    ref_model=ref_model,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset=train_data,   # NOT tokenized; PPOTrainer expects raw text fields\n",
    "    data_collator=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prompt_only(ex):\n",
    "    prompt = (\n",
    "        \"You are a helpful math tutor. Solve the following problem step by step.\\n\"\n",
    "        \"Show your reasoning clearly, and end with \\\"#### <answer>\\\".\\n\\n\"\n",
    "        f\"Question:\\n{ex['question']}\\n\\nAnswer:\\n\"\n",
    "    )\n",
    "    return {\"prompt\": prompt, \"answer\": ex[\"answer\"]}\n",
    "\n",
    "ppo_gsm = gsm8k[\"train\"].map(make_prompt_only)\n",
    "ppo_trainer = PPOTrainer(\n",
    "    config=ppo_config,\n",
    "    model=policy_model,\n",
    "    ref_model=ref_model,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset=ppo_gsm,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_final_answer(text: str):\n",
    "    \"\"\"\n",
    "    Extract the final answer (in the form #### <answer>) from the generated text.\n",
    "\n",
    "    Args:\n",
    "        text (str): generated text from the model.\n",
    "\n",
    "    Returns:\n",
    "        The extracted final answer as a string, or None if not found.\n",
    "    \"\"\"\n",
    "    # Look for pattern #### <number>; TODO: handle more complex answers\n",
    "    m = re.search(r\"####\\s*([-+]?\\d+(\\.\\d+)?)\", text)\n",
    "    if m:\n",
    "        return m.group(1)\n",
    "    return None\n",
    "\n",
    "def correctness_reward(generated: str, gold_answer: str) -> float:\n",
    "    \"\"\"\n",
    "    Compute the correctness reward based on final answers. If the final answer\n",
    "    extracted from the generated text matches the gold answer, return 1.0, else 0.0.\n",
    "    \n",
    "    Args:\n",
    "        generated (str): The generated text from the model.\n",
    "        gold_answer (str): The ground truth answer.\n",
    "        \n",
    "    Returns:\n",
    "        float: 1.0 if answers match, else 0.0.\n",
    "    \"\"\"\n",
    "    # TODO: Use Math-Verify for more robust evaluation\n",
    "    gold = extract_final_answer(gold_answer)\n",
    "    pred = extract_final_answer(generated)\n",
    "    if gold is None or pred is None:\n",
    "        return 0.0\n",
    "    return 1.0 if gold.strip() == pred.strip() else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_dataset = ppo_gsm.select(range(2000))  # tiny subset\n",
    "\n",
    "for epoch in range(3):\n",
    "    for batch_start in range(0, len(ppo_dataset), ppo_config.batch_size):\n",
    "        batch = ppo_dataset[batch_start: batch_start + ppo_config.batch_size]\n",
    "        if len(batch[\"prompt\"]) == 0:\n",
    "            continue\n",
    "        \n",
    "        queries = batch[\"prompt\"]          # list[str]\n",
    "        gold_answers = batch[\"answer\"]     # list[str]\n",
    "\n",
    "        # 1. Generate responses\n",
    "        responses = []\n",
    "        for q in queries:\n",
    "            gen = policy_model.generate(\n",
    "                **tokenizer(q, return_tensors=\"pt\").to(policy_model.device),\n",
    "                max_new_tokens=256,\n",
    "                do_sample=True,\n",
    "                top_p=0.9,\n",
    "                temperature=0.7,\n",
    "            )\n",
    "            resp_text = tokenizer.decode(gen[0], skip_special_tokens=True)\n",
    "            # Keep only the completion part if you want; minimal version uses full text\n",
    "            responses.append(resp_text)\n",
    "\n",
    "        # 2. Compute rewards\n",
    "        rewards = []\n",
    "        for r, gold in zip(responses, gold_answers):\n",
    "            rewards.append(correctness_reward(r, gold))\n",
    "        \n",
    "        # 3. PPO update\n",
    "        stats = ppo_trainer.step(queries, responses, rewards)\n",
    "        # optionally log stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, tokenizer, ds, num_samples=500):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    subset = ds.select(range(min(num_samples, len(ds))))\n",
    "\n",
    "    for ex in tqdm(subset):\n",
    "        prompt = (\n",
    "            \"You are a helpful math tutor. Solve the following problem step by step.\\n\"\n",
    "            \"Show your reasoning clearly, and end with \\\"#### <answer>\\\".\\n\\n\"\n",
    "            f\"Question:\\n{ex['question']}\\n\\nAnswer:\\n\"\n",
    "        )\n",
    "        gold = ex[\"answer\"]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            gen = model.generate(\n",
    "                **tokenizer(prompt, return_tensors=\"pt\").to(model.device),\n",
    "                max_new_tokens=256,\n",
    "                do_sample=False,      # greedy for eval\n",
    "            )\n",
    "        out = tokenizer.decode(gen[0], skip_special_tokens=True)\n",
    "        # depending on your format, maybe slice out just the answer:\n",
    "        pred = out[len(prompt):]\n",
    "\n",
    "        r = correctness_reward(pred, gold)\n",
    "        correct += r\n",
    "        total += 1\n",
    "\n",
    "    acc = correct / total\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsm_test = gsm8k[\"test\"]\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ").to(device)\n",
    "\n",
    "acc_base = evaluate_model(base_model, tokenizer, gsm_test)\n",
    "acc_sft  = evaluate_model(sft_model, tokenizer, gsm_test)\n",
    "acc_ppo  = evaluate_model(policy_model, tokenizer, gsm_test)         # PPO-only\n",
    "# acc_sft_ppo = evaluate_model(policy_model_sft, tokenizer, gsm_test)  # SFT→PPO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl-lora",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
