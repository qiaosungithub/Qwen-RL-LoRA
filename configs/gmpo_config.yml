# GMPO Training Configuration
device: 'cuda:0'
cache_dir: '/home/zehua/6.4610/Qwen-RL-LoRA/'
wandb_notes: 'GMPO training - geometric mean policy optimization on GSM8K'

model:
    name: Qwen/Qwen3-8B

dataset:
    name: gsmhard  # Can be 'gsm8k', 'gsmhard', or 'svamp'
    max_length: 512

lora:
    rank: 16
    alpha: 32
    dropout: 0.0

# GMPO uses TRL's GRPO framework with geometric mean
grpo:
    # Training hyperparameters
    output_dir: qwen-gmpo-gsm8k
    logging_steps: 1
    per_device_train_batch_size: 8
    gradient_accumulation_steps: 1
    num_generations: 8  # Group size for GRPO/GMPO
    max_prompt_length: 512
    max_completion_length: 512
    learning_rate: 5.0e-6

    # GMPO-specific: wider clipping range for better exploration
    # Standard GRPO uses (0.8, 1.2), GMPO uses (e^-0.4, e^0.4) â‰ˆ (0.67, 1.49)
    epsilon_low: 0.33  # This will be used as 1 - epsilon_low
    epsilon_high: 0.49  # This will be used as 1 + epsilon_high

    # Reward scaling
    scale_rewards: group  # 'group', 'batch', or 'none'

    # Training control
    max_steps: 200
    save_strategy: steps
    save_steps: 50

    # Mixed precision
    fp16: false
    bf16: true

    # Reporting
    report_to: wandb

# Generation settings for rewards
generation:
    temperature: 0.6
    top_p: 0.95
    top_k: 20
    do_sample: true
    max_new_tokens: 512

# Reward function weights
reward:
    format_weight: 1.0      # Weight for format reward (using XML tags)
    correctness_weight: 1.0  # Weight for correctness reward
