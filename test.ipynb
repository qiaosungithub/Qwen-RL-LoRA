{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Notebook\n",
    "\n",
    "Implement a simple SFT and PPO training pipeline for finetuning Qwen2.5-7B model on GSM8K dataset.\n",
    "\n",
    "1. Load the Qwen2.5-7B model and tokenizer.\n",
    "2. Load the GSM8K dataset from `openai/gsm8k`.\n",
    "3. Split the dataset into training and validation sets.\n",
    "4. Implement Supervised Fine-Tuning (SFT) on the training set using peft (LoRA).\n",
    "5. Implement Proximal Policy Optimization (PPO) on the SFT model using trl."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U transformers datasets evaluate peft trl bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from datasets import load_dataset\n",
    "from trl import AutoModelForCausalLMWithValueHead, PPOConfig, PPOTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.padding_side = \"right\"\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],  # adjust to Qwen’s naming\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsm8k = load_dataset(\"openai/gsm8k\", \"main\")\n",
    "\n",
    "def format_example(ex):\n",
    "    prompt = (\n",
    "        \"You are a helpful math tutor. Solve the following problem step by step.\\n\"\n",
    "        \"Show your reasoning clearly, and put the final answer in the form \\\"#### <answer>\\\".\\n\\n\"\n",
    "        f\"Question:\\n{ex['question']}\\n\\nAnswer:\\n\"\n",
    "    )\n",
    "    # GSM8K answer already ends with '#### <ans>'\n",
    "    target = ex[\"answer\"]\n",
    "    full_text = prompt + target\n",
    "    return {\"text\": full_text}\n",
    "\n",
    "gsm8k = gsm8k.map(format_example)\n",
    "train_data = gsm8k[\"train\"]\n",
    "test_data = gsm8k[\"test\"]\n",
    "\n",
    "def tokenize_fn(ex):\n",
    "    out = tokenizer(\n",
    "        ex[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=1024,\n",
    "    )\n",
    "    out[\"labels\"] = out[\"input_ids\"].copy()\n",
    "    return out\n",
    "\n",
    "tokenized_train = train_data.map(tokenize_fn, batched=True, remove_columns=train_data.column_names)\n",
    "# tokenized_train = tokenized_train.select(range(5000))  # Optionally limit to first 5000 samples for quicker training\n",
    "tokenized_test = test_data.map(tokenize_fn, batched=True, remove_columns=test_data.column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Fine-Tuning (SFT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./qwen-gsm8k-sft\",\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=8,\n",
    "    num_train_epochs=2,\n",
    "    learning_rate=2e-4,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.03,\n",
    "    bf16=True,\n",
    "    logging_steps=20,\n",
    "    save_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./qwen-gsm8k-sft-lora\")\n",
    "tokenizer.save_pretrained(\"./qwen-gsm8k-sft-lora\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proximal Policy Optimization (PPO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "\n",
    "# policy model with value head\n",
    "policy_model = AutoModelForCausalLMWithValueHead.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# reference model (frozen)\n",
    "ref_model = AutoModelForCausalLMWithValueHead.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "ref_model.eval()\n",
    "for p in ref_model.parameters():\n",
    "    p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "ppo_config = PPOConfig(\n",
    "    batch_size=64,          # queries per PPO step\n",
    "    forward_batch_size=16,  # microbatching\n",
    "    learning_rate=1e-5,\n",
    "    log_with=None,          # wandb if you want\n",
    "    mini_batch_size=16,\n",
    "    ppo_epochs=4,\n",
    "    kl_penalty=\"kl\",\n",
    "    kl_coef=0.1,\n",
    "    target_kl=0.1,\n",
    ")\n",
    "\n",
    "ppo_trainer = PPOTrainer(\n",
    "    config=ppo_config,\n",
    "    model=policy_model,\n",
    "    ref_model=ref_model,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset=train_data,   # NOT tokenized; PPOTrainer expects raw text fields\n",
    "    data_collator=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prompt_only(ex):\n",
    "    prompt = (\n",
    "        \"You are a helpful math tutor. Solve the following problem step by step.\\n\"\n",
    "        \"Show your reasoning clearly, and end with \\\"#### <answer>\\\".\\n\\n\"\n",
    "        f\"Question:\\n{ex['question']}\\n\\nAnswer:\\n\"\n",
    "    )\n",
    "    return {\"prompt\": prompt, \"answer\": ex[\"answer\"]}\n",
    "\n",
    "ppo_gsm = gsm8k[\"train\"].map(make_prompt_only)\n",
    "ppo_trainer = PPOTrainer(\n",
    "    config=ppo_config,\n",
    "    model=policy_model,\n",
    "    ref_model=ref_model,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset=ppo_gsm,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_final_answer(text: str):\n",
    "    # Look for pattern #### <number>; TODO: handle more complex answers\n",
    "    m = re.search(r\"####\\s*([-+]?\\d+(\\.\\d+)?)\", text)\n",
    "    if m:\n",
    "        return m.group(1)\n",
    "    return None\n",
    "\n",
    "def correctness_reward(generated: str, gold_answer: str) -> float:\n",
    "    gold = extract_final_answer(gold_answer)\n",
    "    pred = extract_final_answer(generated)\n",
    "    if gold is None or pred is None:\n",
    "        return 0.0\n",
    "    return 1.0 if gold.strip() == pred.strip() else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_dataset = ppo_gsm.select(range(2000))  # tiny subset\n",
    "\n",
    "for epoch in range(3):\n",
    "    for batch_start in range(0, len(ppo_dataset), ppo_config.batch_size):\n",
    "        batch = ppo_dataset[batch_start: batch_start + ppo_config.batch_size]\n",
    "        if len(batch[\"prompt\"]) == 0:\n",
    "            continue\n",
    "        \n",
    "        queries = batch[\"prompt\"]          # list[str]\n",
    "        gold_answers = batch[\"answer\"]     # list[str]\n",
    "\n",
    "        # 1. Generate responses\n",
    "        responses = []\n",
    "        for q in queries:\n",
    "            gen = policy_model.generate(\n",
    "                **tokenizer(q, return_tensors=\"pt\").to(policy_model.device),\n",
    "                max_new_tokens=256,\n",
    "                do_sample=True,\n",
    "                top_p=0.9,\n",
    "                temperature=0.7,\n",
    "            )\n",
    "            resp_text = tokenizer.decode(gen[0], skip_special_tokens=True)\n",
    "            # Keep only the completion part if you want; minimal version uses full text\n",
    "            responses.append(resp_text)\n",
    "\n",
    "        # 2. Compute rewards\n",
    "        rewards = []\n",
    "        for r, gold in zip(responses, gold_answers):\n",
    "            rewards.append(correctness_reward(r, gold))\n",
    "        \n",
    "        # 3. PPO update\n",
    "        stats = ppo_trainer.step(queries, responses, rewards)\n",
    "        # optionally log stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, tokenizer, ds, num_samples=500):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    subset = ds.select(range(min(num_samples, len(ds))))\n",
    "\n",
    "    for ex in tqdm(subset):\n",
    "        prompt = (\n",
    "            \"You are a helpful math tutor. Solve the following problem step by step.\\n\"\n",
    "            \"Show your reasoning clearly, and end with \\\"#### <answer>\\\".\\n\\n\"\n",
    "            f\"Question:\\n{ex['question']}\\n\\nAnswer:\\n\"\n",
    "        )\n",
    "        gold = ex[\"answer\"]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            gen = model.generate(\n",
    "                **tokenizer(prompt, return_tensors=\"pt\").to(model.device),\n",
    "                max_new_tokens=256,\n",
    "                do_sample=False,      # greedy for eval\n",
    "            )\n",
    "        out = tokenizer.decode(gen[0], skip_special_tokens=True)\n",
    "        # depending on your format, maybe slice out just the answer:\n",
    "        pred = out[len(prompt):]\n",
    "\n",
    "        r = correctness_reward(pred, gold)\n",
    "        correct += r\n",
    "        total += 1\n",
    "\n",
    "    acc = correct / total\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsm_test = gsm8k[\"test\"]\n",
    "\n",
    "acc_base = evaluate_model(base_model, tokenizer, gsm_test)\n",
    "acc_sft  = evaluate_model(sft_model, tokenizer, gsm_test)\n",
    "acc_ppo  = evaluate_model(policy_model, tokenizer, gsm_test)         # PPO-only\n",
    "acc_sft_ppo = evaluate_model(policy_model_sft, tokenizer, gsm_test)  # SFT→PPO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl-lora",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
